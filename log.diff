diff --git a/OGD_DELAY.m b/OGD_DELAY.m
index d178952..fe0805f 100644
--- a/OGD_DELAY.m
+++ b/OGD_DELAY.m
@@ -20,7 +20,7 @@ D = 1;
 global eta;
 eta = 0;
 global gzs;
-gzs  = zeros(1,T); % <G , Z>
+gzs  = zeros(1,T+1); % <G , Z>
 % output variable
 global regrets;
 regrets = zeros(1,T);
@@ -40,6 +40,8 @@ diff = zeros(1,T);
 global y;
 y = 0.5;
 
+global feedbackHeap;
+feedbackHeap = MinHeap(T);
 %%%%%%%%%%%%%%%%%%
 % main function  %
 %%%%%%%%%%%%%%%%%%
@@ -57,10 +59,11 @@ function doubling(M)
   myChoices=zeros(1,M);
   regrets=zeros(1,M);
   regrets_div_t=zeros(1,M);
-  rng(1);
+%   rng(1);
+rng('shuffle');
   for m = 1 : M
 %    [myChoices(m),experts(m), regrets(m)]=
-    iteration(2^(m-1),2^(m)-1,true);
+    iteration(2^(m-1),2^(m)-1,false);
 %    regrets_div_t(m) = regrets(m)/2^m;
   end
   
@@ -108,30 +111,7 @@ function out = OGD_Primary(T)
   
   
 end
-% the difference of reward function U
-function uout = gradient(x_t,gz,eta,G)
-  uout = - G(1)*(G(1)*x_t - gz - eta);  
-end
-% the reward function U
-function uout = Ut(x_t,gz,eta,G)
-  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
-end
 
-function uout = Ut_expert(u,gzs,eta,t,G)
-  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
-end
-%the projection funciton
-function x_t = project(y_t,x_bound)
-  if x_bound(1) <= y_t && y_t <= x_bound(2)
-    x_t = y_t;
-  else
-    if y_t < x_bound(1)
-      x_t = x_bound(1);
-    else
-      x_t = x_bound(2);
-    end
-  end
-end
 
 function iteration(t_b,t_e,doubling_flag)
   global G;
@@ -147,42 +127,79 @@ function iteration(t_b,t_e,doubling_flag)
   global myChoices;
   global myRewards;
   global y;
+  global feedbackHeap;
   u = 0;
   eta1 = 0;
- 
-%   slot_end = t_e - t_b + 1;
-
-  for t = t_b : t_e
+  feedbackCount = 0;
+  
+  % start at 0
     Z(1:end) = D * rand(size(Z,1),1);
-    gzs(t) = G(2:end) * Z;
-    % caculate u
-    if t == 1
-      u =  (gzs(t) + eta) / G(1);
-    else
-      u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
-    end
-    u = project(u,x_bound);
-    experts(t) = u;
-    x_t = project(y,x_bound); 
+    gzs(1) = G(2:end) * Z;
+  %
+  for t = t_b : t_e 
+    % my choice
     if doubling_flag 
-      eta1 = t_b + 1;
+      eta1 = t_b;
     else
       eta1 = t+1;
     end
-    
+    x_t = project(y,x_bound);
     y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
     
     myChoices(t) = x_t;
+    % my rewards
     if t ==1
       myRewards(t)  = Ut(x_t,gzs(t),eta,G);
     else
       myRewards(t)  =myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
     end
 
+    
+    % caculate expert choice
+    if t == 1
+      u =  (gzs(t) + eta) / G(1);
+    else
+      u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
+    end
+    
+    u = project(u,x_bound);
+    experts(t) = u;
+    
+    % expert rewards
     expertsRewards(t) = Ut_expert(experts(t),gzs,eta,t,G);
+    
     regrets(t) = myRewards(t) - expertsRewards(t);
     regrets_div_t(t) = regrets(t) / t;
+    %%%
+    
+    Z(1:end) = D * rand(size(Z,1),1);
+    gzs(t+1) = G(2:end) * Z;
+  end
+end
 
+
+% the difference of reward function U
+function uout = gradient(x_t,gz,eta,G)
+  uout = - G(1)*(G(1)*x_t - gz - eta);  
+end
+% the reward function U
+function uout = Ut(x_t,gz,eta,G)
+  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
+end
+
+function uout = Ut_expert(u,gzs,eta,t,G)
+  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
+end
+%the projection funciton
+function x_t = project(y_t,x_bound)
+  if x_bound(1) <= y_t && y_t <= x_bound(2)
+    x_t = y_t;
+  else
+    if y_t < x_bound(1)
+      x_t = x_bound(1);
+    else
+      x_t = x_bound(2);
+    end
   end
 end
 
diff --git a/OGD_DELAY.m~ b/OGD_DELAY.m~
index 5051290..5fe81c5 100644
--- a/OGD_DELAY.m~
+++ b/OGD_DELAY.m~
@@ -20,7 +20,7 @@ D = 1;
 global eta;
 eta = 0;
 global gzs;
-gzs  = zeros(1,T); % <G , Z>
+gzs  = zeros(1,T+1); % <G , Z>
 % output variable
 global regrets;
 regrets = zeros(1,T);
@@ -40,6 +40,8 @@ diff = zeros(1,T);
 global y;
 y = 0.5;
 
+global feedbackHeap;
+feedbackHeap = MinHeap(T);
 %%%%%%%%%%%%%%%%%%
 % main function  %
 %%%%%%%%%%%%%%%%%%
@@ -108,30 +110,7 @@ function out = OGD_Primary(T)
   
   
 end
-% the difference of reward function U
-function uout = gradient(x_t,gz,eta,G)
-  uout = - G(1)*(G(1)*x_t - gz - eta);  
-end
-% the reward function U
-function uout = Ut(x_t,gz,eta,G)
-  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
-end
 
-function uout = Ut_new(u,gzs,eta,t,G)
-  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
-end
-% the projection funciton
-function x_t = project(y_t,x_bound)
-  if x_bound(1) <= y_t && y_t <= x_bound(2)
-    x_t = y_t;
-  else
-    if y_t < x_bound(1)
-      x_t = x_bound(1);
-    else
-      x_t = x_bound(2);
-    end
-  end
-end
 
 function iteration(t_b,t_e,doubling_flag)
   global G;
@@ -147,40 +126,87 @@ function iteration(t_b,t_e,doubling_flag)
   global myChoices;
   global myRewards;
   global y;
+  global feedbackHeap;
   u = 0;
   eta1 = 0;
-%   slot_end = t_e - t_b + 1;
-
+  feedbackCount = 0;
+  
+  % start at 0
+  
+  %
   for t = t_b : t_e
     Z(1:end) = D * rand(size(Z,1),1);
     gzs(t) = G(2:end) * Z;
-    % caculate u
+    
+    
+    % my choice
+    if doubling_flag 
+      eta1 = t_b + 1;
+    else
+      eta1 = t+1;
+    end
+  
+    y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
+    x_t = project(y,x_bound); 
+    myChoices(t) = x_t;
+    % my rewards
+    if t ==1
+      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
+    else
+      myRewards(t)  =myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
+    end
+
+    
+    % caculate expert choice
     if t == 1
       u =  (gzs(t) + eta) / G(1);
     else
       u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
     end
+    
     u = project(u,x_bound);
     experts(t) = u;
-    x_t = project(y,x_bound); 
-    if doubling_flag 
-      eta1 = t_b + 1;
-    else
-      eta1 = t+1;
-    end
     
-    y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
+    % expert rewards
+    expertsRewards(t) = Ut_expert(experts(t),gzs,eta,t,G);
     
-    myChoices(t) = x_t;
-    myRewards(t)  = Ut(x_t,gzs(t),eta,G);
-    expertsRewards(t) = Ut_new(experts(t),gzs,eta,t,G);
-    regrets(t) = sum(myRewards(1:t)) - expertsRewards(t);
+    regrets(t) = myRewards(t) - expertsRewards(t);
     regrets_div_t(t) = regrets(t) / t;
+    %%%
+  end
+end
+
 
+% the difference of reward function U
+function uout = gradient(x_t,gz,eta,G)
+  uout = - G(1)*(G(1)*x_t - gz - eta);  
+end
+% the reward function U
+function uout = Ut(x_t,gz,eta,G)
+  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
+end
+
+function uout = Ut_expert(u,gzs,eta,t,G)
+  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
+end
+%the projection funciton
+function x_t = project(y_t,x_bound)
+  if x_bound(1) <= y_t && y_t <= x_bound(2)
+    x_t = y_t;
+  else
+    if y_t < x_bound(1)
+      x_t = x_bound(1);
+    else
+      x_t = x_bound(2);
+    end
   end
 end
 
 
+%%%%%%%
+%     %
+%%%%%%%
+
 function [feedbackTime,gz] = boundDelay(t,B)
   global G;
   global Z;
@@ -204,3 +230,11 @@ function [feedbackTime,gz] = logDelay(t)
   feedbackTime = t + ceil(log2(t));
   gz =  G(2:end) * Z;
 end
+
+function [feedbackTime,gz] = squareDelay(t)
+  global G;
+  global Z;
+  Z(1:end) = D * rand(size(Z,1),1);
+  feedbackTime = t^T;
+  gz =  G(2:end) * Z;
+end
