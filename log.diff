diff --git a/OGD.m b/OGD.m
index f228c8c..3a4240b 100644
--- a/OGD.m
+++ b/OGD.m
@@ -1,5 +1,5 @@
 %use doubling tricking to iterate
-M = 14; % 2 ^ 15 = 32768
+M = 12; % 2 ^ 15 = 32768
 % the maxiums turn will iterate T times;
 T = 2^(M) - 1;
 % T = 50000;
diff --git a/OGD_DELAY.m b/OGD_DELAY.m
index d178952..e427587 100644
--- a/OGD_DELAY.m
+++ b/OGD_DELAY.m
@@ -1,9 +1,9 @@
 function OMG_DELAY()
   import MinHeap
 %use doubling tricking to iterate
-M = 14; % 2 ^ 15 = 32768
+M = 12; % 2 ^ 15 = 32768
 % the maxiums turn will iterate T times;
-T = 2^(M) - 1;
+T = 2^(M)-1; % avoid the last value to 0
 % T = 50000;
 % G is  positive retional number begin with 1
 N = 100; % N is used to set G and Z
@@ -20,7 +20,7 @@ D = 1;
 global eta;
 eta = 0;
 global gzs;
-gzs  = zeros(1,T); % <G , Z>
+gzs  = zeros(1,T+1); % <G , Z>
 % output variable
 global regrets;
 regrets = zeros(1,T);
@@ -40,6 +40,8 @@ diff = zeros(1,T);
 global y;
 y = 0.5;
 
+global feedbackHeap;
+feedbackHeap = MinHeap(T);
 %%%%%%%%%%%%%%%%%%
 % main function  %
 %%%%%%%%%%%%%%%%%%
@@ -53,11 +55,10 @@ function doubling(M)
   global experts;
   global myChoices;
   global regrets;
-  experts=zeros(1,M);
-  myChoices=zeros(1,M);
-  regrets=zeros(1,M);
-  regrets_div_t=zeros(1,M);
-  rng(1);
+  global myRewards;
+  global expertsRewards;
+   rng(1);
+%rng('shuffle');
   for m = 1 : M
 %    [myChoices(m),experts(m), regrets(m)]=
     iteration(2^(m-1),2^(m)-1,true);
@@ -90,6 +91,7 @@ function out = OGD_Primary(T)
   global myChoices;
   global regrets;
   
+   
   disp('Begin Loop');
   fprintf('Iterate %d turns',T);
   iteration(1,T,false);
@@ -108,30 +110,7 @@ function out = OGD_Primary(T)
   
   
 end
-% the difference of reward function U
-function uout = gradient(x_t,gz,eta,G)
-  uout = - G(1)*(G(1)*x_t - gz - eta);  
-end
-% the reward function U
-function uout = Ut(x_t,gz,eta,G)
-  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
-end
 
-function uout = Ut_expert(u,gzs,eta,t,G)
-  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
-end
-%the projection funciton
-function x_t = project(y_t,x_bound)
-  if x_bound(1) <= y_t && y_t <= x_bound(2)
-    x_t = y_t;
-  else
-    if y_t < x_bound(1)
-      x_t = x_bound(1);
-    else
-      x_t = x_bound(2);
-    end
-  end
-end
 
 function iteration(t_b,t_e,doubling_flag)
   global G;
@@ -147,42 +126,84 @@ function iteration(t_b,t_e,doubling_flag)
   global myChoices;
   global myRewards;
   global y;
+  global feedbackHeap;
+  
   u = 0;
   eta1 = 0;
- 
-%   slot_end = t_e - t_b + 1;
-
-  for t = t_b : t_e
+  feedbackCount = 0;
+  
+  % start at 0
     Z(1:end) = D * rand(size(Z,1),1);
-    gzs(t) = G(2:end) * Z;
-    % caculate u
-    if t == 1
-      u =  (gzs(t) + eta) / G(1);
-    else
-      u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
-    end
-    u = project(u,x_bound);
-    experts(t) = u;
-    x_t = project(y,x_bound); 
+    gzs(1) = G(2:end) * Z;
+  %
+  for t = t_b : t_e 
+%     Z(1:end) = D * rand(size(Z,1),1);
+%     gzs(t) = G(2:end) * Z;
+    % my choice
     if doubling_flag 
-      eta1 = t_b + 1;
+      eta1 = t_b;
     else
       eta1 = t+1;
     end
-    
+    x_t = project(y,x_bound);
     y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
     
     myChoices(t) = x_t;
-    if t ==1
-      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
+    % my rewards
+    if t ~=1
+      myRewards(t)  = myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
     else
-      myRewards(t)  =myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
+      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
     end
 
+    
+    % caculate expert choice
+    if t ~= 1
+      u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
+    else
+      u =  (gzs(t) + eta) / G(1);
+    end
+    
+    u = project(u,x_bound);
+    experts(t) = u;
+    
+    % expert rewards
     expertsRewards(t) = Ut_expert(experts(t),gzs,eta,t,G);
+    
     regrets(t) = myRewards(t) - expertsRewards(t);
     regrets_div_t(t) = regrets(t) / t;
+    %%%
+    
+    Z(1:end) = D * rand(size(Z,1),1);
+    gzs(t+1) = G(2:end) * Z;
+  end
+  
+  
+end
+
+
+% the difference of reward function U
+function uout = gradient(x_t,gz,eta,G)
+  uout = - G(1)*(G(1)*x_t - gz - eta);  
+end
+% the reward function U
+function uout = Ut(x_t,gz,eta,G)
+  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
+end
 
+function uout = Ut_expert(u,gzs,eta,t,G)
+  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
+end
+%the projection funciton
+function x_t = project(y_t,x_bound)
+  if x_bound(1) <= y_t && y_t <= x_bound(2)
+    x_t = y_t;
+  else
+    if y_t < x_bound(1)
+      x_t = x_bound(1);
+    else
+      x_t = x_bound(2);
+    end
   end
 end
 
diff --git a/OGD_DELAY.m~ b/OGD_DELAY.m~
index 5051290..f2b215b 100644
--- a/OGD_DELAY.m~
+++ b/OGD_DELAY.m~
@@ -20,7 +20,7 @@ D = 1;
 global eta;
 eta = 0;
 global gzs;
-gzs  = zeros(1,T); % <G , Z>
+gzs  = zeros(1,T+1); % <G , Z>
 % output variable
 global regrets;
 regrets = zeros(1,T);
@@ -40,6 +40,8 @@ diff = zeros(1,T);
 global y;
 y = 0.5;
 
+global feedbackHeap;
+feedbackHeap = MinHeap(T);
 %%%%%%%%%%%%%%%%%%
 % main function  %
 %%%%%%%%%%%%%%%%%%
@@ -57,10 +59,11 @@ function doubling(M)
   myChoices=zeros(1,M);
   regrets=zeros(1,M);
   regrets_div_t=zeros(1,M);
-  rng(1);
+   rng(1);
+%rng('shuffle');
   for m = 1 : M
 %    [myChoices(m),experts(m), regrets(m)]=
-    iteration(2^(m-1),2^(m)-1,true);
+    iteration(2^(m-1),2^(m)-1,false);
 %    regrets_div_t(m) = regrets(m)/2^m;
   end
   
@@ -108,30 +111,7 @@ function out = OGD_Primary(T)
   
   
 end
-% the difference of reward function U
-function uout = gradient(x_t,gz,eta,G)
-  uout = - G(1)*(G(1)*x_t - gz - eta);  
-end
-% the reward function U
-function uout = Ut(x_t,gz,eta,G)
-  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
-end
 
-function uout = Ut_new(u,gzs,eta,t,G)
-  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
-end
-% the projection funciton
-function x_t = project(y_t,x_bound)
-  if x_bound(1) <= y_t && y_t <= x_bound(2)
-    x_t = y_t;
-  else
-    if y_t < x_bound(1)
-      x_t = x_bound(1);
-    else
-      x_t = x_bound(2);
-    end
-  end
-end
 
 function iteration(t_b,t_e,doubling_flag)
   global G;
@@ -147,40 +127,89 @@ function iteration(t_b,t_e,doubling_flag)
   global myChoices;
   global myRewards;
   global y;
+  global feedbackHeap;
   u = 0;
   eta1 = 0;
-%   slot_end = t_e - t_b + 1;
-
-  for t = t_b : t_e
+  feedbackCount = 0;
+  
+  % start at 0
     Z(1:end) = D * rand(size(Z,1),1);
-    gzs(t) = G(2:end) * Z;
-    % caculate u
+    gzs(1) = G(2:end) * Z;
+  %
+  for t = t_b : t_e 
+%     Z(1:end) = D * rand(size(Z,1),1);
+%     gzs(t) = G(2:end) * Z;
+    % my choice
+    if doubling_flag 
+      eta1 = t_b;
+    else
+      eta1 = t+1;
+    end
+    x_t = project(y,x_bound);
+    y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
+    
+    myChoices(t) = x_t;
+    % my rewards
+    if t ==1
+      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
+    else
+      myRewards(t)  =myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
+    end
+
+    
+    % caculate expert choice
     if t == 1
       u =  (gzs(t) + eta) / G(1);
     else
       u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
     end
+    
     u = project(u,x_bound);
     experts(t) = u;
-    x_t = project(y,x_bound); 
-    if doubling_flag 
-      eta1 = t_b + 1;
-    else
-      eta1 = t+1;
-    end
     
-    y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
+    % expert rewards
+    expertsRewards(t) = Ut_expert(experts(t),gzs,eta,t,G);
     
-    myChoices(t) = x_t;
-    myRewards(t)  = Ut(x_t,gzs(t),eta,G);
-    expertsRewards(t) = Ut_new(experts(t),gzs,eta,t,G);
-    regrets(t) = sum(myRewards(1:t)) - expertsRewards(t);
+    regrets(t) = myRewards(t) - expertsRewards(t);
     regrets_div_t(t) = regrets(t) / t;
+    %%%
+    
+    Z(1:end) = D * rand(size(Z,1),1);
+    gzs(t+1) = G(2:end) * Z;
+  end
+end
+
+
+% the difference of reward function U
+function uout = gradient(x_t,gz,eta,G)
+  uout = - G(1)*(G(1)*x_t - gz - eta);  
+end
+% the reward function U
+function uout = Ut(x_t,gz,eta,G)
+  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
+end
 
+function uout = Ut_expert(u,gzs,eta,t,G)
+  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
+end
+%the projection funciton
+function x_t = project(y_t,x_bound)
+  if x_bound(1) <= y_t && y_t <= x_bound(2)
+    x_t = y_t;
+  else
+    if y_t < x_bound(1)
+      x_t = x_bound(1);
+    else
+      x_t = x_bound(2);
+    end
   end
 end
 
 
+%%%%%%%
+%     %
+%%%%%%%
+
 function [feedbackTime,gz] = boundDelay(t,B)
   global G;
   global Z;
@@ -204,3 +233,11 @@ function [feedbackTime,gz] = logDelay(t)
   feedbackTime = t + ceil(log2(t));
   gz =  G(2:end) * Z;
 end
+
+function [feedbackTime,gz] = squareDelay(t)
+  global G;
+  global Z;
+  Z(1:end) = D * rand(size(Z,1),1);
+  feedbackTime = t^T;
+  gz =  G(2:end) * Z;
+end
diff --git a/log.diff b/log.diff
new file mode 100644
index 0000000..2b72815
--- /dev/null
+++ b/log.diff
@@ -0,0 +1,351 @@
+diff --git a/OGD.m b/OGD.m
+index f228c8c..3a4240b 100644
+--- a/OGD.m
++++ b/OGD.m
+@@ -1,5 +1,5 @@
+ %use doubling tricking to iterate
+-M = 14; % 2 ^ 15 = 32768
++M = 12; % 2 ^ 15 = 32768
+ % the maxiums turn will iterate T times;
+ T = 2^(M) - 1;
+ % T = 50000;
+diff --git a/OGD_DELAY.m b/OGD_DELAY.m
+index d178952..e427587 100644
+--- a/OGD_DELAY.m
++++ b/OGD_DELAY.m
+@@ -1,9 +1,9 @@
+ function OMG_DELAY()
+   import MinHeap
+ %use doubling tricking to iterate
+-M = 14; % 2 ^ 15 = 32768
++M = 12; % 2 ^ 15 = 32768
+ % the maxiums turn will iterate T times;
+-T = 2^(M) - 1;
++T = 2^(M)-1; % avoid the last value to 0
+ % T = 50000;
+ % G is  positive retional number begin with 1
+ N = 100; % N is used to set G and Z
+@@ -20,7 +20,7 @@ D = 1;
+ global eta;
+ eta = 0;
+ global gzs;
+-gzs  = zeros(1,T); % <G , Z>
++gzs  = zeros(1,T+1); % <G , Z>
+ % output variable
+ global regrets;
+ regrets = zeros(1,T);
+@@ -40,6 +40,8 @@ diff = zeros(1,T);
+ global y;
+ y = 0.5;
+ 
++global feedbackHeap;
++feedbackHeap = MinHeap(T);
+ %%%%%%%%%%%%%%%%%%
+ % main function  %
+ %%%%%%%%%%%%%%%%%%
+@@ -53,11 +55,10 @@ function doubling(M)
+   global experts;
+   global myChoices;
+   global regrets;
+-  experts=zeros(1,M);
+-  myChoices=zeros(1,M);
+-  regrets=zeros(1,M);
+-  regrets_div_t=zeros(1,M);
+-  rng(1);
++  global myRewards;
++  global expertsRewards;
++   rng(1);
++%rng('shuffle');
+   for m = 1 : M
+ %    [myChoices(m),experts(m), regrets(m)]=
+     iteration(2^(m-1),2^(m)-1,true);
+@@ -90,6 +91,7 @@ function out = OGD_Primary(T)
+   global myChoices;
+   global regrets;
+   
++   
+   disp('Begin Loop');
+   fprintf('Iterate %d turns',T);
+   iteration(1,T,false);
+@@ -108,30 +110,7 @@ function out = OGD_Primary(T)
+   
+   
+ end
+-% the difference of reward function U
+-function uout = gradient(x_t,gz,eta,G)
+-  uout = - G(1)*(G(1)*x_t - gz - eta);  
+-end
+-% the reward function U
+-function uout = Ut(x_t,gz,eta,G)
+-  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
+-end
+ 
+-function uout = Ut_expert(u,gzs,eta,t,G)
+-  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
+-end
+-%the projection funciton
+-function x_t = project(y_t,x_bound)
+-  if x_bound(1) <= y_t && y_t <= x_bound(2)
+-    x_t = y_t;
+-  else
+-    if y_t < x_bound(1)
+-      x_t = x_bound(1);
+-    else
+-      x_t = x_bound(2);
+-    end
+-  end
+-end
+ 
+ function iteration(t_b,t_e,doubling_flag)
+   global G;
+@@ -147,42 +126,84 @@ function iteration(t_b,t_e,doubling_flag)
+   global myChoices;
+   global myRewards;
+   global y;
++  global feedbackHeap;
++  
+   u = 0;
+   eta1 = 0;
+- 
+-%   slot_end = t_e - t_b + 1;
+-
+-  for t = t_b : t_e
++  feedbackCount = 0;
++  
++  % start at 0
+     Z(1:end) = D * rand(size(Z,1),1);
+-    gzs(t) = G(2:end) * Z;
+-    % caculate u
+-    if t == 1
+-      u =  (gzs(t) + eta) / G(1);
+-    else
+-      u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
+-    end
+-    u = project(u,x_bound);
+-    experts(t) = u;
+-    x_t = project(y,x_bound); 
++    gzs(1) = G(2:end) * Z;
++  %
++  for t = t_b : t_e 
++%     Z(1:end) = D * rand(size(Z,1),1);
++%     gzs(t) = G(2:end) * Z;
++    % my choice
+     if doubling_flag 
+-      eta1 = t_b + 1;
++      eta1 = t_b;
+     else
+       eta1 = t+1;
+     end
+-    
++    x_t = project(y,x_bound);
+     y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
+     
+     myChoices(t) = x_t;
+-    if t ==1
+-      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
++    % my rewards
++    if t ~=1
++      myRewards(t)  = myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
+     else
+-      myRewards(t)  =myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
++      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
+     end
+ 
++    
++    % caculate expert choice
++    if t ~= 1
++      u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
++    else
++      u =  (gzs(t) + eta) / G(1);
++    end
++    
++    u = project(u,x_bound);
++    experts(t) = u;
++    
++    % expert rewards
+     expertsRewards(t) = Ut_expert(experts(t),gzs,eta,t,G);
++    
+     regrets(t) = myRewards(t) - expertsRewards(t);
+     regrets_div_t(t) = regrets(t) / t;
++    %%%
++    
++    Z(1:end) = D * rand(size(Z,1),1);
++    gzs(t+1) = G(2:end) * Z;
++  end
++  
++  
++end
++
++
++% the difference of reward function U
++function uout = gradient(x_t,gz,eta,G)
++  uout = - G(1)*(G(1)*x_t - gz - eta);  
++end
++% the reward function U
++function uout = Ut(x_t,gz,eta,G)
++  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
++end
+ 
++function uout = Ut_expert(u,gzs,eta,t,G)
++  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
++end
++%the projection funciton
++function x_t = project(y_t,x_bound)
++  if x_bound(1) <= y_t && y_t <= x_bound(2)
++    x_t = y_t;
++  else
++    if y_t < x_bound(1)
++      x_t = x_bound(1);
++    else
++      x_t = x_bound(2);
++    end
+   end
+ end
+ 
+diff --git a/OGD_DELAY.m~ b/OGD_DELAY.m~
+index 5051290..f2b215b 100644
+--- a/OGD_DELAY.m~
++++ b/OGD_DELAY.m~
+@@ -20,7 +20,7 @@ D = 1;
+ global eta;
+ eta = 0;
+ global gzs;
+-gzs  = zeros(1,T); % <G , Z>
++gzs  = zeros(1,T+1); % <G , Z>
+ % output variable
+ global regrets;
+ regrets = zeros(1,T);
+@@ -40,6 +40,8 @@ diff = zeros(1,T);
+ global y;
+ y = 0.5;
+ 
++global feedbackHeap;
++feedbackHeap = MinHeap(T);
+ %%%%%%%%%%%%%%%%%%
+ % main function  %
+ %%%%%%%%%%%%%%%%%%
+@@ -57,10 +59,11 @@ function doubling(M)
+   myChoices=zeros(1,M);
+   regrets=zeros(1,M);
+   regrets_div_t=zeros(1,M);
+-  rng(1);
++   rng(1);
++%rng('shuffle');
+   for m = 1 : M
+ %    [myChoices(m),experts(m), regrets(m)]=
+-    iteration(2^(m-1),2^(m)-1,true);
++    iteration(2^(m-1),2^(m)-1,false);
+ %    regrets_div_t(m) = regrets(m)/2^m;
+   end
+   
+@@ -108,30 +111,7 @@ function out = OGD_Primary(T)
+   
+   
+ end
+-% the difference of reward function U
+-function uout = gradient(x_t,gz,eta,G)
+-  uout = - G(1)*(G(1)*x_t - gz - eta);  
+-end
+-% the reward function U
+-function uout = Ut(x_t,gz,eta,G)
+-  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
+-end
+ 
+-function uout = Ut_new(u,gzs,eta,t,G)
+-  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
+-end
+-% the projection funciton
+-function x_t = project(y_t,x_bound)
+-  if x_bound(1) <= y_t && y_t <= x_bound(2)
+-    x_t = y_t;
+-  else
+-    if y_t < x_bound(1)
+-      x_t = x_bound(1);
+-    else
+-      x_t = x_bound(2);
+-    end
+-  end
+-end
+ 
+ function iteration(t_b,t_e,doubling_flag)
+   global G;
+@@ -147,40 +127,89 @@ function iteration(t_b,t_e,doubling_flag)
+   global myChoices;
+   global myRewards;
+   global y;
++  global feedbackHeap;
+   u = 0;
+   eta1 = 0;
+-%   slot_end = t_e - t_b + 1;
+-
+-  for t = t_b : t_e
++  feedbackCount = 0;
++  
++  % start at 0
+     Z(1:end) = D * rand(size(Z,1),1);
+-    gzs(t) = G(2:end) * Z;
+-    % caculate u
++    gzs(1) = G(2:end) * Z;
++  %
++  for t = t_b : t_e 
++%     Z(1:end) = D * rand(size(Z,1),1);
++%     gzs(t) = G(2:end) * Z;
++    % my choice
++    if doubling_flag 
++      eta1 = t_b;
++    else
++      eta1 = t+1;
++    end
++    x_t = project(y,x_bound);
++    y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
++    
++    myChoices(t) = x_t;
++    % my rewards
++    if t ==1
++      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
++    else
++      myRewards(t)  =myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
++    end
++
++    
++    % caculate expert choice
+     if t == 1
+       u =  (gzs(t) + eta) / G(1);
+     else
+       u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
+     end
++    
+     u = project(u,x_bound);
+     experts(t) = u;
+-    x_t = project(y,x_bound); 
+-    if doubling_flag 
+-      eta1 = t_b + 1;
+-    else
+-      eta1 = t+1;
+-    end
+     
+-    y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
++    % expert rewards
++    expertsRewards(t) = Ut_expert(experts(t),gzs,eta,t,G);
+     
+-    myChoices(t) = x_t;
+-    myRewards(t)  = Ut(x_t,gzs(t),eta,G);
+-    expertsRewards(t) = Ut_new(experts(t),gzs,eta,t,G);
+-    regrets(t) = sum(myRewards(1:t)) - expertsRewards(t);
++    regrets(t) = myRewards(t) - expertsRewards(t);
+     regrets_div_t(t) = regrets(t) / t;
++    %%%
++    
++    Z(1:end) = D * rand(size(Z,1),1);
++    gzs(t+1) = G(2:end) * Z;
++  end
++end
++
++
++% the difference of reward function U
++function uout = gradient(x_t,gz,eta,G)
++  uout = - G(1)*(G(1)*x_t - gz - eta);  
++end
++% the reward function U
++function uout = Ut(x_t,gz,eta,G)
++  uout = -0.5 * (G
\ No newline at end of file
