diff --git a/OGD.m b/OGD.m
index 5e83f63..47015d9 100644
--- a/OGD.m
+++ b/OGD.m
@@ -1,6 +1,6 @@
 function out = OMG()
 %use doubling tricking to iterate
-M = 5; % 2 ^ 15 = 32768
+M = 11; % 2 ^ 15 = 32768
 % the maxiums turn will iterate T times;
 T = 2^(M)-1; % avoid the last value to 0
 % T = 50000;
@@ -44,8 +44,17 @@ feedbackHeap = MinHeap(T+1,ones(1,4)* inf);
 %%%%%%%%%%%%%%%%%%
 % out=doubling(M);
 % regrets = zeros(1,T);
-  out = OGD_Primary(T);
-% 
+rng(1);
+  out_s = OGD_Primary(T);
+  rng(1);
+  regret_s=ogdfix(8);
+  rng(1);
+  figure('name','RG','NumberTitle','off','Position',[0,500,700,500]);
+  plot(out_s,'DisplayName','out_s');
+  hold on;
+  plot(regret_s,'DisplayName','regret_s');
+  legend('out_s','regret_s');
+  hold off;
 %  figure('name','Regrets','NumberTitle','off','Position',[100,0,700,500]);
 %  plot(out,'DisplayName','doubling');
 %  hold on;
@@ -85,7 +94,7 @@ function out = doubling(M)
   end
   
    rng(1);
-% regret_s=ogddoublingtrick(M-1);
+   
   
   figure('name','The value of Xt','NumberTitle','off','Position',[0,500,700,500]);
   plot(experts,'DisplayName','experts');
@@ -95,7 +104,8 @@ function out = doubling(M)
   hold off;
   figure('name','The aluve of regret','NumberTitle','off','Position',[700,500,700,500]);
   hold on;
-%   plot(regret_s'+ones(1,size(regret_s,1))*89);
+%    plot(regret_s'+ones(1,size(regret_s,1))*89);
+   plot(regret_s);
   plot(regrets);
   hold off;
 
@@ -238,4 +248,76 @@ function x_t = project(y_t,x_bound)
       x_t = x_bound(2);
     end
   end
-end
\ No newline at end of file
+end
+
+
+function regret=ogdfix(y0)
+%x belongs to [0,1000]
+%y0=8;
+
+  global regrets_div_t;
+  global experts;
+  global myChoices;
+  global regrets;
+  global myRewards;
+  global expertsRewards;
+  global gzs;
+  
+T=2^11-1;
+eta=1;
+n=100;
+s=zeros(T+1,1);
+
+y=zeros(T,1);
+% this y has some problem!!!
+% I fix it
+%calculate y(1)
+z=zeros(n-1,1);
+
+x0=project(y0,[0,1000]);
+
+% y(1)=y0-(x0-sum0-eta);
+y(1) = y0;
+% regret
+x=zeros(T,1);
+z=rand(n-1,1);
+s(1)=sum(z);
+u=zeros(T,1);
+user=zeros(T,1);
+expert_reward=zeros(T,1);
+users_reward=zeros(T,1);
+regret=zeros(T,1);
+for t=1:T
+    %user's choice
+%     for i=2:n
+%         z(i)=rand(1);
+%     end
+    z=rand(n-1,1);
+    x(t)=project(y(t),[0,1000]);
+    s(t+1)=sum(z);
+    y(t+1)=y(t)-(x(t)-s(t)-eta)/(t+1);
+    %user performance
+    users_reward(t)=-0.5*(x(t)-s(t)-eta)^2; 
+    if t==1
+        user(t)=users_reward(t);
+    else
+        user(t)=user(t-1)+users_reward(t);
+    %best expert performance
+    end
+    if t==1
+        u(t)=s(t)+eta;
+    else
+        u(t)=u(t-1)*(t-1)/t+(s(t)+eta)/t; 
+    end
+    for j=1:t
+        expert_reward(t)=expert_reward(t)-0.5*(u(t)-s(j)-eta)^2;
+    end
+    %calculate regret
+    regret(t)=user(t)-expert_reward(t);
+    end
+end
+
+
+
+
+
diff --git a/OGD.m~ b/OGD.m~
index 13904f2..76e22c4 100644
--- a/OGD.m~
+++ b/OGD.m~
@@ -1,6 +1,6 @@
 function out = OMG()
 %use doubling tricking to iterate
-M = 15; % 2 ^ 15 = 32768
+M = 11; % 2 ^ 15 = 32768
 % the maxiums turn will iterate T times;
 T = 2^(M)-1; % avoid the last value to 0
 % T = 50000;
@@ -38,14 +38,23 @@ global y;
 y = 8;
 
 global feedbackHeap;
-feedbackHeap = MinHeap(T);
+feedbackHeap = MinHeap(T+1,ones(1,4)* inf);
 %%%%%%%%%%%%%%%%%%
 % main function  %
 %%%%%%%%%%%%%%%%%%
 % out=doubling(M);
 % regrets = zeros(1,T);
-  out = OGD_Primary(T);
-% 
+rng(1);
+  out_s = OGD_Primary(T);
+  rng(1);
+  regret_s=ogdfix(8);
+  rng(1);
+  figure('name','RG','NumberTitle','off','Position',[0,500,700,500]);
+  plot(out_s,'DisplayName','out_s');
+  hold on;
+  plot(regret_s,'DisplayName','regret_s');
+  legend('out_s','regret_s');
+  hold off;
 %  figure('name','Regrets','NumberTitle','off','Position',[100,0,700,500]);
 %  plot(out,'DisplayName','doubling');
 %  hold on;
@@ -85,7 +94,7 @@ function out = doubling(M)
   end
   
    rng(1);
-% regret_s=ogddoublingtrick(M-1);
+   
   
   figure('name','The value of Xt','NumberTitle','off','Position',[0,500,700,500]);
   plot(experts,'DisplayName','experts');
@@ -95,7 +104,8 @@ function out = doubling(M)
   hold off;
   figure('name','The aluve of regret','NumberTitle','off','Position',[700,500,700,500]);
   hold on;
-%   plot(regret_s'+ones(1,size(regret_s,1))*89);
+%    plot(regret_s'+ones(1,size(regret_s,1))*89);
+   plot(regret_s);
   plot(regrets);
   hold off;
 
@@ -238,4 +248,66 @@ function x_t = project(y_t,x_bound)
       x_t = x_bound(2);
     end
   end
-end
\ No newline at end of file
+end
+
+
+function regret=ogdfix(y0)
+%x belongs to [0,1000]
+%y0=8;
+T=2^11-1;
+eta=1;
+n=100;
+s=zeros(T,1);
+%calculate y(1)
+z=zeros(n-1,1);
+for i=2:n
+    z(i)=rand(1);
+end
+x0=project(y0,[0,1000]);
+sum0=sum(z);
+y(1)=y0-(x0-sum0-eta);
+
+% regret
+x=zeros(T,1);
+y=zeros(T,1);
+u=zeros(T,1);
+user=zeros(T,1);
+expert_reward=zeros(T,1);
+users_reward=zeros(T,1);
+regret=zeros(T,1);
+for t=1:T
+    %user's choice
+    for i=2:n
+        z(i)=rand(1);
+    end
+    z(2:)
+    x(t)=project(y(t),[0,1000]);
+    s(t)=sum(z);
+    y(t+1)=y(t)-(x(t)-s(t)-eta)/(t+1);
+    %user performance
+    users_reward(t)=-0.5*(x(t)-s(t)-eta)^2; 
+    if t==1
+        user(t)=users_reward(t);
+    else
+        user(t)=user(t-1)+users_reward(t);
+    %best expert performance
+    if t==1
+        u(t)=s(t)+eta;
+    else
+        u(t)=u(t-1)*(t-1)/t+(s(t)+eta)/t; 
+    end
+    for j=1:t
+        expert_reward(t)=expert_reward(t)-0.5*(u(t)-s(j)-eta)^2;
+    end
+    %calculate regret
+    regret(t)=user(t)-expert_reward(t);
+    end
+
+
+
+end
+end
+
+
+
+
diff --git a/log.diff b/log.diff
index ec86c36..93c38bd 100644
--- a/log.diff
+++ b/log.diff
@@ -1,747 +0,0 @@
-diff --git a/OGD.m b/OGD.m
-index f228c8c..3a4240b 100644
---- a/OGD.m
-+++ b/OGD.m
-@@ -1,5 +1,5 @@
- %use doubling tricking to iterate
--M = 14; % 2 ^ 15 = 32768
-+M = 12; % 2 ^ 15 = 32768
- % the maxiums turn will iterate T times;
- T = 2^(M) - 1;
- % T = 50000;
-diff --git a/OGD_DELAY.m b/OGD_DELAY.m
-index d178952..e427587 100644
---- a/OGD_DELAY.m
-+++ b/OGD_DELAY.m
-@@ -1,9 +1,9 @@
- function OMG_DELAY()
-   import MinHeap
- %use doubling tricking to iterate
--M = 14; % 2 ^ 15 = 32768
-+M = 12; % 2 ^ 15 = 32768
- % the maxiums turn will iterate T times;
--T = 2^(M) - 1;
-+T = 2^(M)-1; % avoid the last value to 0
- % T = 50000;
- % G is  positive retional number begin with 1
- N = 100; % N is used to set G and Z
-@@ -20,7 +20,7 @@ D = 1;
- global eta;
- eta = 0;
- global gzs;
--gzs  = zeros(1,T); % <G , Z>
-+gzs  = zeros(1,T+1); % <G , Z>
- % output variable
- global regrets;
- regrets = zeros(1,T);
-@@ -40,6 +40,8 @@ diff = zeros(1,T);
- global y;
- y = 0.5;
- 
-+global feedbackHeap;
-+feedbackHeap = MinHeap(T);
- %%%%%%%%%%%%%%%%%%
- % main function  %
- %%%%%%%%%%%%%%%%%%
-@@ -53,11 +55,10 @@ function doubling(M)
-   global experts;
-   global myChoices;
-   global regrets;
--  experts=zeros(1,M);
--  myChoices=zeros(1,M);
--  regrets=zeros(1,M);
--  regrets_div_t=zeros(1,M);
--  rng(1);
-+  global myRewards;
-+  global expertsRewards;
-+   rng(1);
-+%rng('shuffle');
-   for m = 1 : M
- %    [myChoices(m),experts(m), regrets(m)]=
-     iteration(2^(m-1),2^(m)-1,true);
-@@ -90,6 +91,7 @@ function out = OGD_Primary(T)
-   global myChoices;
-   global regrets;
-   
-+   
-   disp('Begin Loop');
-   fprintf('Iterate %d turns',T);
-   iteration(1,T,false);
-@@ -108,30 +110,7 @@ function out = OGD_Primary(T)
-   
-   
- end
--% the difference of reward function U
--function uout = gradient(x_t,gz,eta,G)
--  uout = - G(1)*(G(1)*x_t - gz - eta);  
--end
--% the reward function U
--function uout = Ut(x_t,gz,eta,G)
--  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
--end
- 
--function uout = Ut_expert(u,gzs,eta,t,G)
--  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
--end
--%the projection funciton
--function x_t = project(y_t,x_bound)
--  if x_bound(1) <= y_t && y_t <= x_bound(2)
--    x_t = y_t;
--  else
--    if y_t < x_bound(1)
--      x_t = x_bound(1);
--    else
--      x_t = x_bound(2);
--    end
--  end
--end
- 
- function iteration(t_b,t_e,doubling_flag)
-   global G;
-@@ -147,42 +126,84 @@ function iteration(t_b,t_e,doubling_flag)
-   global myChoices;
-   global myRewards;
-   global y;
-+  global feedbackHeap;
-+  
-   u = 0;
-   eta1 = 0;
-- 
--%   slot_end = t_e - t_b + 1;
--
--  for t = t_b : t_e
-+  feedbackCount = 0;
-+  
-+  % start at 0
-     Z(1:end) = D * rand(size(Z,1),1);
--    gzs(t) = G(2:end) * Z;
--    % caculate u
--    if t == 1
--      u =  (gzs(t) + eta) / G(1);
--    else
--      u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
--    end
--    u = project(u,x_bound);
--    experts(t) = u;
--    x_t = project(y,x_bound); 
-+    gzs(1) = G(2:end) * Z;
-+  %
-+  for t = t_b : t_e 
-+%     Z(1:end) = D * rand(size(Z,1),1);
-+%     gzs(t) = G(2:end) * Z;
-+    % my choice
-     if doubling_flag 
--      eta1 = t_b + 1;
-+      eta1 = t_b;
-     else
-       eta1 = t+1;
-     end
--    
-+    x_t = project(y,x_bound);
-     y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
-     
-     myChoices(t) = x_t;
--    if t ==1
--      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
-+    % my rewards
-+    if t ~=1
-+      myRewards(t)  = myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
-     else
--      myRewards(t)  =myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
-+      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
-     end
- 
-+    
-+    % caculate expert choice
-+    if t ~= 1
-+      u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
-+    else
-+      u =  (gzs(t) + eta) / G(1);
-+    end
-+    
-+    u = project(u,x_bound);
-+    experts(t) = u;
-+    
-+    % expert rewards
-     expertsRewards(t) = Ut_expert(experts(t),gzs,eta,t,G);
-+    
-     regrets(t) = myRewards(t) - expertsRewards(t);
-     regrets_div_t(t) = regrets(t) / t;
-+    %%%
-+    
-+    Z(1:end) = D * rand(size(Z,1),1);
-+    gzs(t+1) = G(2:end) * Z;
-+  end
-+  
-+  
-+end
-+
-+
-+% the difference of reward function U
-+function uout = gradient(x_t,gz,eta,G)
-+  uout = - G(1)*(G(1)*x_t - gz - eta);  
-+end
-+% the reward function U
-+function uout = Ut(x_t,gz,eta,G)
-+  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
-+end
- 
-+function uout = Ut_expert(u,gzs,eta,t,G)
-+  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
-+end
-+%the projection funciton
-+function x_t = project(y_t,x_bound)
-+  if x_bound(1) <= y_t && y_t <= x_bound(2)
-+    x_t = y_t;
-+  else
-+    if y_t < x_bound(1)
-+      x_t = x_bound(1);
-+    else
-+      x_t = x_bound(2);
-+    end
-   end
- end
- 
-diff --git a/OGD_DELAY.m~ b/OGD_DELAY.m~
-index 5051290..f2b215b 100644
---- a/OGD_DELAY.m~
-+++ b/OGD_DELAY.m~
-@@ -20,7 +20,7 @@ D = 1;
- global eta;
- eta = 0;
- global gzs;
--gzs  = zeros(1,T); % <G , Z>
-+gzs  = zeros(1,T+1); % <G , Z>
- % output variable
- global regrets;
- regrets = zeros(1,T);
-@@ -40,6 +40,8 @@ diff = zeros(1,T);
- global y;
- y = 0.5;
- 
-+global feedbackHeap;
-+feedbackHeap = MinHeap(T);
- %%%%%%%%%%%%%%%%%%
- % main function  %
- %%%%%%%%%%%%%%%%%%
-@@ -57,10 +59,11 @@ function doubling(M)
-   myChoices=zeros(1,M);
-   regrets=zeros(1,M);
-   regrets_div_t=zeros(1,M);
--  rng(1);
-+   rng(1);
-+%rng('shuffle');
-   for m = 1 : M
- %    [myChoices(m),experts(m), regrets(m)]=
--    iteration(2^(m-1),2^(m)-1,true);
-+    iteration(2^(m-1),2^(m)-1,false);
- %    regrets_div_t(m) = regrets(m)/2^m;
-   end
-   
-@@ -108,30 +111,7 @@ function out = OGD_Primary(T)
-   
-   
- end
--% the difference of reward function U
--function uout = gradient(x_t,gz,eta,G)
--  uout = - G(1)*(G(1)*x_t - gz - eta);  
--end
--% the reward function U
--function uout = Ut(x_t,gz,eta,G)
--  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
--end
- 
--function uout = Ut_new(u,gzs,eta,t,G)
--  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
--end
--% the projection funciton
--function x_t = project(y_t,x_bound)
--  if x_bound(1) <= y_t && y_t <= x_bound(2)
--    x_t = y_t;
--  else
--    if y_t < x_bound(1)
--      x_t = x_bound(1);
--    else
--      x_t = x_bound(2);
--    end
--  end
--end
- 
- function iteration(t_b,t_e,doubling_flag)
-   global G;
-@@ -147,40 +127,89 @@ function iteration(t_b,t_e,doubling_flag)
-   global myChoices;
-   global myRewards;
-   global y;
-+  global feedbackHeap;
-   u = 0;
-   eta1 = 0;
--%   slot_end = t_e - t_b + 1;
--
--  for t = t_b : t_e
-+  feedbackCount = 0;
-+  
-+  % start at 0
-     Z(1:end) = D * rand(size(Z,1),1);
--    gzs(t) = G(2:end) * Z;
--    % caculate u
-+    gzs(1) = G(2:end) * Z;
-+  %
-+  for t = t_b : t_e 
-+%     Z(1:end) = D * rand(size(Z,1),1);
-+%     gzs(t) = G(2:end) * Z;
-+    % my choice
-+    if doubling_flag 
-+      eta1 = t_b;
-+    else
-+      eta1 = t+1;
-+    end
-+    x_t = project(y,x_bound);
-+    y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
-+    
-+    myChoices(t) = x_t;
-+    % my rewards
-+    if t ==1
-+      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
-+    else
-+      myRewards(t)  =myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
-+    end
-+
-+    
-+    % caculate expert choice
-     if t == 1
-       u =  (gzs(t) + eta) / G(1);
-     else
-       u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
-     end
-+    
-     u = project(u,x_bound);
-     experts(t) = u;
--    x_t = project(y,x_bound); 
--    if doubling_flag 
--      eta1 = t_b + 1;
--    else
--      eta1 = t+1;
--    end
-     
--    y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
-+    % expert rewards
-+    expertsRewards(t) = Ut_expert(experts(t),gzs,eta,t,G);
-     
--    myChoices(t) = x_t;
--    myRewards(t)  = Ut(x_t,gzs(t),eta,G);
--    expertsRewards(t) = Ut_new(experts(t),gzs,eta,t,G);
--    regrets(t) = sum(myRewards(1:t)) - expertsRewards(t);
-+    regrets(t) = myRewards(t) - expertsRewards(t);
-     regrets_div_t(t) = regrets(t) / t;
-+    %%%
-+    
-+    Z(1:end) = D * rand(size(Z,1),1);
-+    gzs(t+1) = G(2:end) * Z;
-+  end
-+end
-+
-+
-+% the difference of reward function U
-+function uout = gradient(x_t,gz,eta,G)
-+  uout = - G(1)*(G(1)*x_t - gz - eta);  
-+end
-+% the reward function U
-+function uout = Ut(x_t,gz,eta,G)
-+  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
-+end
- 
-+function uout = Ut_expert(u,gzs,eta,t,G)
-+  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
-+end
-+%the projection funciton
-+function x_t = project(y_t,x_bound)
-+  if x_bound(1) <= y_t && y_t <= x_bound(2)
-+    x_t = y_t;
-+  else
-+    if y_t < x_bound(1)
-+      x_t = x_bound(1);
-+    else
-+      x_t = x_bound(2);
-+    end
-   end
- end
- 
- 
-+%%%%%%%
-+%     %
-+%%%%%%%
-+
- function [feedbackTime,gz] = boundDelay(t,B)
-   global G;
-   global Z;
-@@ -204,3 +233,11 @@ function [feedbackTime,gz] = logDelay(t)
-   feedbackTime = t + ceil(log2(t));
-   gz =  G(2:end) * Z;
- end
-+
-+function [feedbackTime,gz] = squareDelay(t)
-+  global G;
-+  global Z;
-+  Z(1:end) = D * rand(size(Z,1),1);
-+  feedbackTime = t^T;
-+  gz =  G(2:end) * Z;
-+end
-diff --git a/log.diff b/log.diff
-new file mode 100644
-index 0000000..2b72815
---- /dev/null
-+++ b/log.diff
-@@ -0,0 +1,351 @@
-+diff --git a/OGD.m b/OGD.m
-+index f228c8c..3a4240b 100644
-+--- a/OGD.m
-++++ b/OGD.m
-+@@ -1,5 +1,5 @@
-+ %use doubling tricking to iterate
-+-M = 14; % 2 ^ 15 = 32768
-++M = 12; % 2 ^ 15 = 32768
-+ % the maxiums turn will iterate T times;
-+ T = 2^(M) - 1;
-+ % T = 50000;
-+diff --git a/OGD_DELAY.m b/OGD_DELAY.m
-+index d178952..e427587 100644
-+--- a/OGD_DELAY.m
-++++ b/OGD_DELAY.m
-+@@ -1,9 +1,9 @@
-+ function OMG_DELAY()
-+   import MinHeap
-+ %use doubling tricking to iterate
-+-M = 14; % 2 ^ 15 = 32768
-++M = 12; % 2 ^ 15 = 32768
-+ % the maxiums turn will iterate T times;
-+-T = 2^(M) - 1;
-++T = 2^(M)-1; % avoid the last value to 0
-+ % T = 50000;
-+ % G is  positive retional number begin with 1
-+ N = 100; % N is used to set G and Z
-+@@ -20,7 +20,7 @@ D = 1;
-+ global eta;
-+ eta = 0;
-+ global gzs;
-+-gzs  = zeros(1,T); % <G , Z>
-++gzs  = zeros(1,T+1); % <G , Z>
-+ % output variable
-+ global regrets;
-+ regrets = zeros(1,T);
-+@@ -40,6 +40,8 @@ diff = zeros(1,T);
-+ global y;
-+ y = 0.5;
-+ 
-++global feedbackHeap;
-++feedbackHeap = MinHeap(T);
-+ %%%%%%%%%%%%%%%%%%
-+ % main function  %
-+ %%%%%%%%%%%%%%%%%%
-+@@ -53,11 +55,10 @@ function doubling(M)
-+   global experts;
-+   global myChoices;
-+   global regrets;
-+-  experts=zeros(1,M);
-+-  myChoices=zeros(1,M);
-+-  regrets=zeros(1,M);
-+-  regrets_div_t=zeros(1,M);
-+-  rng(1);
-++  global myRewards;
-++  global expertsRewards;
-++   rng(1);
-++%rng('shuffle');
-+   for m = 1 : M
-+ %    [myChoices(m),experts(m), regrets(m)]=
-+     iteration(2^(m-1),2^(m)-1,true);
-+@@ -90,6 +91,7 @@ function out = OGD_Primary(T)
-+   global myChoices;
-+   global regrets;
-+   
-++   
-+   disp('Begin Loop');
-+   fprintf('Iterate %d turns',T);
-+   iteration(1,T,false);
-+@@ -108,30 +110,7 @@ function out = OGD_Primary(T)
-+   
-+   
-+ end
-+-% the difference of reward function U
-+-function uout = gradient(x_t,gz,eta,G)
-+-  uout = - G(1)*(G(1)*x_t - gz - eta);  
-+-end
-+-% the reward function U
-+-function uout = Ut(x_t,gz,eta,G)
-+-  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
-+-end
-+ 
-+-function uout = Ut_expert(u,gzs,eta,t,G)
-+-  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
-+-end
-+-%the projection funciton
-+-function x_t = project(y_t,x_bound)
-+-  if x_bound(1) <= y_t && y_t <= x_bound(2)
-+-    x_t = y_t;
-+-  else
-+-    if y_t < x_bound(1)
-+-      x_t = x_bound(1);
-+-    else
-+-      x_t = x_bound(2);
-+-    end
-+-  end
-+-end
-+ 
-+ function iteration(t_b,t_e,doubling_flag)
-+   global G;
-+@@ -147,42 +126,84 @@ function iteration(t_b,t_e,doubling_flag)
-+   global myChoices;
-+   global myRewards;
-+   global y;
-++  global feedbackHeap;
-++  
-+   u = 0;
-+   eta1 = 0;
-+- 
-+-%   slot_end = t_e - t_b + 1;
-+-
-+-  for t = t_b : t_e
-++  feedbackCount = 0;
-++  
-++  % start at 0
-+     Z(1:end) = D * rand(size(Z,1),1);
-+-    gzs(t) = G(2:end) * Z;
-+-    % caculate u
-+-    if t == 1
-+-      u =  (gzs(t) + eta) / G(1);
-+-    else
-+-      u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
-+-    end
-+-    u = project(u,x_bound);
-+-    experts(t) = u;
-+-    x_t = project(y,x_bound); 
-++    gzs(1) = G(2:end) * Z;
-++  %
-++  for t = t_b : t_e 
-++%     Z(1:end) = D * rand(size(Z,1),1);
-++%     gzs(t) = G(2:end) * Z;
-++    % my choice
-+     if doubling_flag 
-+-      eta1 = t_b + 1;
-++      eta1 = t_b;
-+     else
-+       eta1 = t+1;
-+     end
-+-    
-++    x_t = project(y,x_bound);
-+     y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
-+     
-+     myChoices(t) = x_t;
-+-    if t ==1
-+-      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
-++    % my rewards
-++    if t ~=1
-++      myRewards(t)  = myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
-+     else
-+-      myRewards(t)  =myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
-++      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
-+     end
-+ 
-++    
-++    % caculate expert choice
-++    if t ~= 1
-++      u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
-++    else
-++      u =  (gzs(t) + eta) / G(1);
-++    end
-++    
-++    u = project(u,x_bound);
-++    experts(t) = u;
-++    
-++    % expert rewards
-+     expertsRewards(t) = Ut_expert(experts(t),gzs,eta,t,G);
-++    
-+     regrets(t) = myRewards(t) - expertsRewards(t);
-+     regrets_div_t(t) = regrets(t) / t;
-++    %%%
-++    
-++    Z(1:end) = D * rand(size(Z,1),1);
-++    gzs(t+1) = G(2:end) * Z;
-++  end
-++  
-++  
-++end
-++
-++
-++% the difference of reward function U
-++function uout = gradient(x_t,gz,eta,G)
-++  uout = - G(1)*(G(1)*x_t - gz - eta);  
-++end
-++% the reward function U
-++function uout = Ut(x_t,gz,eta,G)
-++  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
-++end
-+ 
-++function uout = Ut_expert(u,gzs,eta,t,G)
-++  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
-++end
-++%the projection funciton
-++function x_t = project(y_t,x_bound)
-++  if x_bound(1) <= y_t && y_t <= x_bound(2)
-++    x_t = y_t;
-++  else
-++    if y_t < x_bound(1)
-++      x_t = x_bound(1);
-++    else
-++      x_t = x_bound(2);
-++    end
-+   end
-+ end
-+ 
-+diff --git a/OGD_DELAY.m~ b/OGD_DELAY.m~
-+index 5051290..f2b215b 100644
-+--- a/OGD_DELAY.m~
-++++ b/OGD_DELAY.m~
-+@@ -20,7 +20,7 @@ D = 1;
-+ global eta;
-+ eta = 0;
-+ global gzs;
-+-gzs  = zeros(1,T); % <G , Z>
-++gzs  = zeros(1,T+1); % <G , Z>
-+ % output variable
-+ global regrets;
-+ regrets = zeros(1,T);
-+@@ -40,6 +40,8 @@ diff = zeros(1,T);
-+ global y;
-+ y = 0.5;
-+ 
-++global feedbackHeap;
-++feedbackHeap = MinHeap(T);
-+ %%%%%%%%%%%%%%%%%%
-+ % main function  %
-+ %%%%%%%%%%%%%%%%%%
-+@@ -57,10 +59,11 @@ function doubling(M)
-+   myChoices=zeros(1,M);
-+   regrets=zeros(1,M);
-+   regrets_div_t=zeros(1,M);
-+-  rng(1);
-++   rng(1);
-++%rng('shuffle');
-+   for m = 1 : M
-+ %    [myChoices(m),experts(m), regrets(m)]=
-+-    iteration(2^(m-1),2^(m)-1,true);
-++    iteration(2^(m-1),2^(m)-1,false);
-+ %    regrets_div_t(m) = regrets(m)/2^m;
-+   end
-+   
-+@@ -108,30 +111,7 @@ function out = OGD_Primary(T)
-+   
-+   
-+ end
-+-% the difference of reward function U
-+-function uout = gradient(x_t,gz,eta,G)
-+-  uout = - G(1)*(G(1)*x_t - gz - eta);  
-+-end
-+-% the reward function U
-+-function uout = Ut(x_t,gz,eta,G)
-+-  uout = -0.5 * (G(1).*x_t - gz - eta).^2;
-+-end
-+ 
-+-function uout = Ut_new(u,gzs,eta,t,G)
-+-  uout = -0.5 * (t * ((G(1)* u - eta)^2 )+sum(-2*(G(1)*u -eta) * gzs(1:t) + gzs(1:t).^2));
-+-end
-+-% the projection funciton
-+-function x_t = project(y_t,x_bound)
-+-  if x_bound(1) <= y_t && y_t <= x_bound(2)
-+-    x_t = y_t;
-+-  else
-+-    if y_t < x_bound(1)
-+-      x_t = x_bound(1);
-+-    else
-+-      x_t = x_bound(2);
-+-    end
-+-  end
-+-end
-+ 
-+ function iteration(t_b,t_e,doubling_flag)
-+   global G;
-+@@ -147,40 +127,89 @@ function iteration(t_b,t_e,doubling_flag)
-+   global myChoices;
-+   global myRewards;
-+   global y;
-++  global feedbackHeap;
-+   u = 0;
-+   eta1 = 0;
-+-%   slot_end = t_e - t_b + 1;
-+-
-+-  for t = t_b : t_e
-++  feedbackCount = 0;
-++  
-++  % start at 0
-+     Z(1:end) = D * rand(size(Z,1),1);
-+-    gzs(t) = G(2:end) * Z;
-+-    % caculate u
-++    gzs(1) = G(2:end) * Z;
-++  %
-++  for t = t_b : t_e 
-++%     Z(1:end) = D * rand(size(Z,1),1);
-++%     gzs(t) = G(2:end) * Z;
-++    % my choice
-++    if doubling_flag 
-++      eta1 = t_b;
-++    else
-++      eta1 = t+1;
-++    end
-++    x_t = project(y,x_bound);
-++    y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
-++    
-++    myChoices(t) = x_t;
-++    % my rewards
-++    if t ==1
-++      myRewards(t)  = Ut(x_t,gzs(t),eta,G);
-++    else
-++      myRewards(t)  =myRewards(t-1) + Ut(x_t,gzs(t),eta,G);
-++    end
-++
-++    
-++    % caculate expert choice
-+     if t == 1
-+       u =  (gzs(t) + eta) / G(1);
-+     else
-+       u = t/(t+1) * experts(t - 1) + 1/(t+1)* 1 /G(1) * (gzs(t) + eta);
-+     end
-++    
-+     u = project(u,x_bound);
-+     experts(t) = u;
-+-    x_t = project(y,x_bound); 
-+-    if doubling_flag 
-+-      eta1 = t_b + 1;
-+-    else
-+-      eta1 = t+1;
-+-    end
-+     
-+-    y = y + (1 / eta1)*(gradient(x_t,gzs(t),eta,G));
-++    % expert rewards
-++    expertsRewards(t) = Ut_expert(experts(t),gzs,eta,t,G);
-+     
-+-    myChoices(t) = x_t;
-+-    myRewards(t)  = Ut(x_t,gzs(t),eta,G);
-+-    expertsRewards(t) = Ut_new(experts(t),gzs,eta,t,G);
-+-    regrets(t) = sum(myRewards(1:t)) - expertsRewards(t);
-++    regrets(t) = myRewards(t) - expertsRewards(t);
-+     regrets_div_t(t) = regrets(t) / t;
-++    %%%
-++    
-++    Z(1:end) = D * rand(size(Z,1),1);
-++    gzs(t+1) = G(2:end) * Z;
-++  end
-++end
-++
-++
-++% the difference of reward function U
-++function uout = gradient(x_t,gz,eta,G)
-++  uout = - G(1)*(G(1)*x_t - gz - eta);  
-++end
-++% the reward function U
-++function uout = Ut(x_t,gz,eta,G)
-++  uout = -0.5 * (G
-\ No newline at end of file
